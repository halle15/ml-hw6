{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/franklin-univ-data-science/comp411/blob/master/Module05_HW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LXPcet9fooe0"
   },
   "source": [
    "## Homework 6\n",
    "\n",
    "1. In the given data set, identify the number of missing values by column; fill the missing values by most frequent number (hint: you need strategy='most_frequent').\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "csv_data = \\\n",
    "'''\n",
    "col1,col2\n",
    ",2.0\n",
    ",\n",
    "10.0,11.0\n",
    "1, 3 \n",
    "1, 11\n",
    "'''\n",
    "\n",
    "df = pd.read_csv(StringIO(csv_data))\n",
    "df\n",
    "```\n",
    "2. Explain the transformer and estimator in sci-kit learn. What are the differences between them?\n",
    "3. In the given data set, generate the dummy variables for the categorical variables using one-hot encoding. \n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(123)\n",
    "\n",
    "df = pd.DataFrame(np.random.rand(10, 2),\n",
    "                 columns = ['col1', 'col2'])\n",
    "df['col3'] = list(''.join(np.random.choice(list('abc')) for _ in range(len(df))))\n",
    "df\n",
    "```\n",
    "4. Assume col3 is the target, separate the original data set in #3 into 80% training and 20% test.\n",
    "5. Load sci-kit learn's diabetes data set by **load_diabetes()**, pick the top 3 most discriminative features using Random Forest (hint: pick the correct method base on the machine learning type)\n",
    "\n",
    "```python\n",
    "\n",
    "from sklearn import datasets\n",
    "diabetes = datasets.load_diabetes()\n",
    "X = diabetes.data\n",
    "y = diabetes.target\n",
    "```\n",
    "6. In two to three paragraphs of prose (i.e. sentences, not bullet lists), summarize and interact with the content that was covered this week in readings and in class meeting. In your summary, you should highlight the major topics, methods, and practices that were covered. Your summary should also interact with the material through personal observations, reflections, and applications to the field of study. In particular, highlight what surprised, enlightened, or otherwise engaged you. In other words, you should think and write critically not just about what was presented but also what you have learned through the session. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SUMMARY \n",
    "This week's lecture, we began with talking about the importance of pre-processing and the quality of the data. Real world data is messy, and a lot of the job of data scientists is preprocessing and ensuring that data is formatted properly to ensure a lack of bias. We began by learning how to identify missing values, and then learned how to quickly drop the values, either by dropping values, or entire columns. We then learned how to perform mean imputation in python in order to fill in missing data. You asked if we should impute before or after splitting training and test data, and I am of the opinion that we should after, as although it may be two separate sets, the mean will be closer to the real mean of the set and thus give better values when imputing if the sample size is smaller, and not represented by the larger sample of the entire set. Although you brought up that we should not modify the test data, as that is supposed to be representative of the \"real world\" dataset, so I was wrong. We learned the difference between ordinal and nominal features. We viewed an issue with some of the \"mapping\" of the colors to numbers, and then you showed us how to create dummy variables to fit data into a ML model. You showed us how to drop one of the dummy variables to reduce multicolinearity, without losing any of the underlying information. You then showed us how to use scikitlearn in order to partition data and quickly assign feature and target variables. We then learned about feature scaling, and why it is necesarry to do it on a gradient descent based model. We also learned the necessity of standardization, and why it might be necesarry to do it. We then went to the next selection and got a better understanding as how to select appropriate features. We then got a better understanding of L1 and L2 regularization. We then recap-ed Random Forests and how features can be optimized for that algorithm. You also showed us how to use a random forest to assess the importance of features, or how much they affect the final result.\n",
    "\n",
    "### HOMEWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  2.],\n",
       "       [ 1., 11.],\n",
       "       [10., 11.],\n",
       "       [ 1.,  3.],\n",
       "       [ 1., 11.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import numpy as np\n",
    "\n",
    "csv_data = \\\n",
    "'''\n",
    "col1,col2\n",
    ",2.0\n",
    ",\n",
    "10.0,11.0\n",
    "1, 3 \n",
    "1, 11\n",
    "'''\n",
    "\n",
    "df = pd.read_csv(StringIO(csv_data))\n",
    "df\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imr = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "imr = imr.fit(df.values)\n",
    "data = imr.transform(df.values)\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0jDnymJhoYIf"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values \n",
    "X_train, X_test, y_train, y_test =train_test_split(X, y, test_size=0.3, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformers are objects or models that transform our dataset into a more applicable dataset for machine learning. An estimator is a ML application that \"fits\" a dataset and is able to extrapolate from data and estimate future inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "      <th>col3_b</th>\n",
       "      <th>col3_c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.696469</td>\n",
       "      <td>0.286139</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.226851</td>\n",
       "      <td>0.551315</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.719469</td>\n",
       "      <td>0.423106</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.980764</td>\n",
       "      <td>0.684830</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.480932</td>\n",
       "      <td>0.392118</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.343178</td>\n",
       "      <td>0.729050</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.438572</td>\n",
       "      <td>0.059678</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.398044</td>\n",
       "      <td>0.737995</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.182492</td>\n",
       "      <td>0.175452</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.531551</td>\n",
       "      <td>0.531828</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       col1      col2  col3_b  col3_c\n",
       "0  0.696469  0.286139       0       0\n",
       "1  0.226851  0.551315       1       0\n",
       "2  0.719469  0.423106       0       0\n",
       "3  0.980764  0.684830       1       0\n",
       "4  0.480932  0.392118       0       0\n",
       "5  0.343178  0.729050       0       0\n",
       "6  0.438572  0.059678       0       0\n",
       "7  0.398044  0.737995       0       1\n",
       "8  0.182492  0.175452       1       0\n",
       "9  0.531551  0.531828       1       0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(123)\n",
    "\n",
    "df = pd.DataFrame(np.random.rand(10, 2),\n",
    "                 columns = ['col1', 'col2'])\n",
    "df['col3'] = list(''.join(np.random.choice(list('abc')) for _ in range(len(df))))\n",
    "df\n",
    "\n",
    "df.columns = [\"col1\", \"col2\", \"col3\"]\n",
    "\n",
    "df = pd.get_dummies(df, drop_first=True) \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['a', 'a', 'c', 'a', 'b', 'c', 'b', 'a'], dtype=object)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df = pd.DataFrame(np.random.rand(10, 2),\n",
    "                 columns = ['col1', 'col2'])\n",
    "df['col3'] = list(''.join(np.random.choice(list('abc')) for _ in range(len(df))))\n",
    "df\n",
    "\n",
    "y, X = df.iloc[:, :2].values , df.iloc[:, 2].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']\n",
      "[0.11206566 0.02722537 0.12250696 0.11258742 0.11073943 0.11381602\n",
      " 0.10916964 0.05892117 0.11894675 0.11402157]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "diabetes = datasets.load_diabetes()\n",
    "X = diabetes.data\n",
    "y = diabetes.target\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=500,\n",
    "                                random_state=1)\n",
    "\n",
    "forest.fit(X, y)\n",
    "importances = forest.feature_importances_\n",
    "    \n",
    "print(diabetes.feature_names)\n",
    "print(importances)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, in order, BMI, Age and then BP are the determining factors for diabetes dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This week, after getting a good overview the previous weeks on some of the different models of machine learning we could use, as well as how to choose proper ones, we took a deeper dive this week into preprocessing data. We made note of the importance of having properly pre-processed data, as issues like categorical as well as missing data, and more, can bias machine learning models into giving incorrect results. \n",
    "    \n",
    "We learned different methods of fixing missing data, like imputation or deletion, as well as how to solve categorical data issues by creating dummy variables.  In our homework, we then used these strategies to fix a dataset up quickly in order to be processed by our machine learning algorithm. We also took time to learn how to properly scale our features, bringing me to a question: what will happen if features are improperly scaled or not at all scaled? We then finished this week by learning how to select meaningful features."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Module05_HW.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "7c45ef34803c71a66af9bd0281108f0d048533fd3cba5a00777b914de484ad0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
